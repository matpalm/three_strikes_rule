# three strikes rule

i have a rule that the third time i explain something to someone i have to write it up...

here's some recent ones...

* [patch based training for convolutional nets](https://colab.research.google.com/drive/1zPrDW8M2c3bFXM26KpeA88ads-7afUsF) : a trivial example of how to structure a fully convolutional net so you can train a 1x1 version and have it operate on SxS
* [reusing model subpaths](https://colab.research.google.com/drive/1XSbW04TjfAF5KVXp8_XZuHtP1L3W707q) : example of reusing keras layers for simple transfer learning
* [measuring baseline random performance for an N way classifier](https://colab.research.google.com/drive/1W08R0r0hjW7i30p-oPO_6hsdcwPi50Yp) : what baseline performance should you expect for an N way classifier given random choice (proportional to support in training set)
* [deriving class_weights from validation data](https://colab.research.google.com/drive/1hS6Wire061HKYtVlnq_Q4W4BBVYyEMa7) : a starting point for class weighting based on validation loss
* [initing the biases in a classifer to closer match training data](https://colab.research.google.com/drive/12K5GnTG79_Usik0U3K2j1FBOH13lh9yN) : how to init your classifier to output a distribution closer to training data distribution at start of training